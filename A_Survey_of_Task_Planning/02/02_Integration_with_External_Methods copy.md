---
marp: true
theme: gaia
paginate: true
backgroundColor: #fff
style: |
  section {
    font-family: "Noto Sans CJK JP", "Noto Sans JP", "IPAexGothic", "Yu Gothic", "Hiragino Sans", sans-serif;
    font-size: 20px;
  }
  h1 {
    font-size: 38px;
    color: #003366;
  }
  h2 {
    font-size: 28px;
    color: #005580;
  }
  strong {
    color: #cc0000;
  }
---

# 壮大なる統合：高度なタスクプランニング能力を実現するLLM拡張戦略

外部手法との統合に関する最新研究動向のサーベイ

- 古典的計画
- 強化学習
- 他分野理論

---

# なぜ「統合」が不可欠なのか？
## スタンドアロンアプローチの限界

### LLMの課題
- 複雑で構造化されたタスクプランニングにおいて、論理的な一貫性や実行可能性の保証が難しい。
- 暗黙的な知識に依存し、明示的な制約や物理法則の厳密な扱いに課題。

### 従来手法の課題
- PDDLのような標準言語は、複雑な現実世界のタスクにおけるモデリング能力と表現力に限界がある。
- 自然言語のような曖昧な要求を直接解釈できず、厳密な形式化が必要。

> 両者の強みを組み合わせることで、単体では解決困難な課題に取り組むことが可能になる。

---

# LLMの能力を拡張する3つの統合アプローチ

1. **古典的計画との統合**  
   形式言語（PDDL）を活用し、プランニングに構造と厳密性をもたらす。

2. **強化学習との統合**  
   フィードバックループを通じて、試行錯誤から最適な意思決定方針を学習する。

3. **他分野理論との融合**  
   認知科学、ゲーム理論、神経科学などの知見を取り入れ、より高度で人間らしいプランニング能力を獲得する。

---

# 古典的計画との統合：構造と形式言語の力を活用する

**Key Concept:** PDDL（Planning Domain Definition Language）を共通言語として、  
LLMの柔軟な言語理解能力と古典的プランナーの厳密な計画生成能力を橋渡しする。

---

# LLMが担う多様な役割：翻訳者からプランナー、そして検証者まで

### Role 1: 翻訳者（Translator）
- 自然言語のタスク記述をPDDL [58-63] や他の論理形式 [64, 65] に変換。
- 古典的プランナーが後続処理を実行。

**注目事例（Notable Example）:**  
- **TWOSTEP [60]**：目標分解を組み合わせ、マルチエージェントプランニングの計画時間を短縮。

### Role 2: プランナー（Planner）
- PDDLを直接解釈してプランを生成 [53, 54]、  
  あるいは自然言語からの変換とプランニングの両方を実行 [66, 67]。

### Role 3: ヒューリスティクス・検証者（Heuristics/Verifier）
- 他のプランナーの探索プロセスを改善するためのヒューリスティクスを提供。
- 生成されたプランのフィードバックに外部検証者を活用。

> Valmeekam et al. [55] の発見：LLMが自律的に実行可能なプランを生成する能力は限定的。  
> しかし、他のプランナーの探索を助けるヒューリスティクスとして、また外部検証者と連携することで、その真価を発揮する。

---

# 高度な統合：動的環境への適応と知識獲得

### エラーからの学習（Learning from Errors）- LASP [56]
- プランナーが実行エラーを起こした際、LLMが観測に基づいて原因を診断。
- 環境との対話を通じて、タスク達成に必要な知識を段階的に構築し、不完全な知識を持つオープンワールド問題に対応。

### フィードバックによる述語学習（Predicate Learning via Feedback）- InterPreT [57]
- PDDLの基本要素である「述語」を、人間の自然言語フィードバックに基づき、GPT-4のようなLLMが反復的に修正・学習するフレームワーク。

### アフォーダンスに基づく計画（Affordance-based Planning）- AutoGPT+P [68]
- オブジェクト検出とChatGPTによる「オブジェクト-アフォーダンス」マッピングを組み合わせ、任意の物体に対する記号的プランニングを可能にする。
- 物理環境における行動の可能性（アフォーダンス）から計画ドメインを導出。

---

# 強化学習との統合：フィードバックループによる意思決定の最適化

**Key Concept:** LLMが持つ汎用的な世界知識や多タスク学習能力と、  
強化学習の強力な意思決定能力を組み合わせ、双方の長所を最大限に引き出す。

---

# MCTSの強化：LLMを世界モデルおよび推論エージェントとして活用

多くのプランニング問題は、強化学習の古典的なタスク形式であるマルコフ決定過程（MDP）として定式化される。  
MCTSはその有力な解法の一つ。

### LLMの役割（Roles of LLM in MCTS）
- **世界モデル（World Model）**：LLMが誘導する世界モデルがMCTSに常識的な事前信念を提供し、探索効率を向上させる（[7], RAP [70]）。
- **価値関数学習（Value Function Learning）**：AlphaZeroに着想を得て、価値関数をファインチューニングし、MCTSのロールアウトプロセスを簡略化・安定化させる（[71]）。
- **In-Context学習・計画（In-Context Learning/Planning）**：MDPのアクター・クリティック更新を模倣し、未知の環境に関する事後理解を更新しながら最適な軌道を生成する（RAFA [74]）。
- **探索と活用の高速化（Rapid Exploration & Exploitation）**：MCTSを核とし、追加の報酬層を組み込むことで、オフラインのログ活用や高速な問題解決を実現する（REX [75]）。

---

# フィードバックの進化：人手からAI、そして環境へ

1. **人間からのフィードバック（RLHF）[77]**
   - LLMを人間の好みに整合させるための標準的な手法。
   - 高品質な人間の嗜好ラベルの収集が重大なボトルネック。

2. **AIからのフィードバック（RLAIF）[78]**
   - 人間の代わりにLLMが嗜好ラベルを付与。
   - RLHFと同等の性能を達成し、スケーラビリティ問題を解決する可能性。
   - 人手への依存を減らし、大規模な最適化を可能にする。

3. **環境からのフィードバック（RLEF）**
   - エージェントが環境と相互作用する中で得られる直接的なフィードバックを活用。
   - Retroformer [79]：環境フィードバックを用いてLLMのプロンプトを改善。  
   - Octopus [80]：環境フィードバック情報で報酬モデルをファインチューニング。

---

# 他分野との融合：新たな視点による能力の拡張

**Key Concept:** タスクプランニングは本質的に学際的な課題であり、  
認知心理学、ゲーム理論、神経科学などの理論を統合することで、  
より効果的で革新的なアプローチが生まれる。

---

# 人間の知性を模倣する：心の理論とゲーム理論からの洞察

### 心の理論（Theory of Mind [98]）
**定義：** 他者の精神状態（思考、信念、欲望）を理解し、行動を予測する能力。

**応用：**
- **K-level reasoning [87]**：相手の視点を再帰的に採用し、次の手を予測する精度を向上。
- **Suspicion-agent [88]**：GPT-4に心の理論を付与し、不完全情報ゲームにおける相手の思考プロセスを予測。

### ゲーム理論（Game Theory）
**定義：** 対立・協調状況下における最適な意思決定を研究する。

**応用：**
- **Alympics [90]**：LLMエージェントを用いて人間行動をシミュレートし、ゲーム理論の研究を進めるプラットフォーム。
- **GAMA(γ)-Bench [93]**：マルチエージェント環境におけるLLMのゲーミング能力を評価するための包括的なフレームワーク。

**発見：** GPT-4でさえ、欲望・信念・行動の合理性において人間とは大きな違いを示す [92]。

---

# 構造とプロセスからの着想：神経科学とグラフ理論

### 神経科学（Neuroscience）
**着想：** 人間の脳機能、特に前頭前野の計画メカニズムや二重過程理論（速い思考と遅い思考）にヒントを得る。

**応用：**
- 複数のGPT-4モジュールで前頭前野の機能を模倣し、多段階推論を強化 [94]。
- 直感的で速い思考（ワンストップ推論）と熟慮的な遅い思考（ToT）を統合した階層的計画アルゴリズムを提案 [95, 96]。

### グラフ理論（Graph Theory）
**着想：** タスクプランニングをグラフ上の経路選択問題として定式化（ノード＝サブタスク、エッジ＝依存関係）。

**応用：Wu et al. [97]**
- LLMが曖昧な要求を解釈し、詳細なステップに分解。
- グラフニューラルネットワーク（GNN）が関連サブタスクを探索。

**特筆すべき点：** GNNの出力をLLMのトークンとして利用する、この分野で初かつ唯一の研究。

---

# 統合がもたらすパラダイムシフト：相乗効果のまとめ

### 1. + 古典的計画
**相乗効果：** LLMが曖昧さを処理し、古典的プランナーが構造と実行可能性を保証する。  
計画生成、検証、探索最適化のツールとして機能。

### 2. + 強化学習
**相乗効果：** LLMが報酬信号生成や世界モデルを提供し、RLがフィードバックを通じて意思決定を最適化する。

### 3. + 他分野理論
**相乗効果：** LLMのタスクプランニングに新たな視点を提供し、ツールを豊かにし、研究の革新的な道を切り拓く。

> スタンドアロンなLLMや古典的なAIプランニングシステムの根源的な限界を克服し、  
> より堅牢で柔軟、かつ効果的なタスクプランニングを実現する。

---

# 残された課題と現在の研究フロンティア

1. **フィードバックのバランス**  
   強化学習において、人間による嗜好ラベルの希少性と、AIフィードバックによる価値整合（Value Alignment）のバランスを取ることは緊急の課題。

2. **スケーラビリティと汎化**  
   LLMと外部モデル間のより深い相互作用を探求し、スケーラビリティと汎化の問題に対処することが現在の研究のホットスポット。

3. **倫理とデータ品質**  
   倫理的配慮、高品質なデータとフィードバックメカニズムの需要は、統合手法を実世界で確実に適用するための重要要素。

---

# 未来への展望：次世代の統合フレームワークと評価指標

## 将来の研究の方向性（Future Research Directions）

### より効率的でスケーラブルな統合フレームワークの開発
タスクプランニングの動的で複雑な性質に対応できる、より柔軟なアーキテクチャの構築。  
新しい手法やモデルを容易に組み込める設計が求められる。

### より良い評価指標の確立
実世界の制約やアプリケーションの動的な性質を考慮した評価指標を開発する。  
これにより、統合アプローチの有効性と実用性をより正確に評価できる。